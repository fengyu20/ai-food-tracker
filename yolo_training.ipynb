{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# YOLO Food Detection Training\n",
        "\n",
        "\n",
        "**Note:** For fast training, it's best to use a GPU runtime in Google Colab. You can upload this notebook to your Colab environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Step 0: Environment Setup\n",
        "\n",
        "**IMPORTANT**: Make sure to enable GPU runtime!\n",
        "- Go to: **Runtime > Change runtime type > Hardware accelerator > GPU**\n",
        "- Choose **T4 GPU** (free tier).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU availability\n",
        "import torch\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
        "else:\n",
        "    print(\"No GPU detected - please enable GPU runtime!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install ultralytics -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import shutil\n",
        "import yaml\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "from ultralytics import YOLO\n",
        "from IPython.display import display, Image, clear_output\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Prepare food image dataset\n",
        "\n",
        "Instead of using the full 3GB dataset, the following cells focus on the smaller dataset (375MB) from the [dataset-ninja website](https://datasetninja.com/food-recognition)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import tarfile\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def download_and_extract(dataset_type=\"sample\", extract_dir=\"/content/extracted_food_recognition\"):\n",
        "\n",
        "    urls = {\n",
        "        \"full\": \"https://assets.supervisely.com/remote/eyJsaW5rIjogImZzOi8vYXNzZXRzLzk1MF9Gb29kIFJlY29nbml0aW9uIDIwMjIvZm9vZC1yZWNvZ25pdGlvbi0yMDIyLURhdGFzZXROaW5qYS50YXIiLCAic2lnIjogIlpqZisyZURmaEoyZkhWNGRiTHBPWkEzN0NodWhlb28wNlZlQXpQQkdBc1U9In0=\",\n",
        "        \"sample\": \"https://assets.supervisely.com/supervisely-supervisely-assets-public/teams_storage/Y/I/Yu/V12n4DX73dwmLY7sz7Bl83qOdACZBbaB9ctVmlEUPBx1qqaqLqnHsLubJQWGAKu3vrqBPty6hRBOrUaXzgPe1jMk7bI1MVcWCNON9vbLeZdPKuRV9Psis3STGSh7.tar\"\n",
        "    }\n",
        "\n",
        "    if dataset_type not in urls:\n",
        "        raise ValueError(\"dataset_type must be either 'full' or 'sample'\")\n",
        "\n",
        "    url = urls[dataset_type]\n",
        "    tar_path = f\"/content/{dataset_type}_food_dataset.tar\"\n",
        "\n",
        "    print(f\"Downloading [{dataset_type}] dataset...\")\n",
        "    !wget -q --show-progress -O \"$tar_path\" \"$url\"\n",
        "\n",
        "    if os.path.exists(extract_dir):\n",
        "        print(f\"{extract_dir} already exists. Skipping extraction.\")\n",
        "        return\n",
        "    \n",
        "    with tarfile.open(tar_path, \"r\") as tar:\n",
        "        members = tar.getmembers()\n",
        "        for member in tqdm(members, desc=\"Extracting\"):\n",
        "            tar.extract(member, path=extract_dir)\n",
        "            \n",
        "\n",
        "    print(f\"Dataset extracted to {extract_dir}\")\n",
        "\n",
        "download_and_extract(\"sample\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Optional: Use dataset-tools for downloading."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install dataset-tools\n",
        "\n",
        "import os\n",
        "import dataset_tools as dtools\n",
        "\n",
        "drive_path = 'content/datasets/FoodRecognition2022'\n",
        "marker_file = os.path.join(drive_path, 'meta.json')  \n",
        "\n",
        "if not os.path.exists(marker_file):\n",
        "    print(\"Downloading dataset...\")\n",
        "    # It will handle the extraction of the dataset automatically.\n",
        "    dtools.download(dataset='Food Recognition 2022', dst_dir=drive_path)\n",
        "else:\n",
        "    print(\"Dataset already exists.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step2: Training Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Filter category with a small number of training images\n",
        "\n",
        "\n",
        "#### 1. Avaliable categories:\n",
        "\n",
        "The class/food category names that are detected in this image dataset can be found in the file `meta.json`:\n",
        "\n",
        "```json\n",
        "{\n",
        "    \"classes\": [\n",
        "        {\n",
        "            \"title\": \"bread-wholemeal\",\n",
        "            \"shape\": \"polygon\",\n",
        "            \"color\": \"#0F8A39\",\n",
        "            \"geometry_config\": {},\n",
        "            \"id\": 3415,\n",
        "            \"hotkey\": \"\"\n",
        "        },\n",
        "        {\n",
        "            \"title\": \"jam\",\n",
        "            \"shape\": \"polygon\",\n",
        "            \"color\": \"#8A460F\",\n",
        "            \"geometry_config\": {},\n",
        "            \"id\": 3417,\n",
        "            \"hotkey\": \"\"\n",
        "        }\n",
        "        ...\n",
        "    ]\n",
        "}\n",
        "```\n",
        "\n",
        "#### 2. Imbalanced food class distribution\n",
        "\n",
        "You can check the food class distribution in [this section](https://datasetninja.com/food-recognition#class-balance).\n",
        "\n",
        "To improve training efficiency, for categories that have fewer than 30 images, this project will skip these categories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# hard code the excluded food categories with less than 30 images\n",
        "excluded_food_categories = set([\n",
        "    \"oil\",\n",
        "    \"tea-spice\",\n",
        "    \"tea-fruit\",\n",
        "    \"tea-ginger\",\n",
        "    \"tea-rooibos\",\n",
        "    \"chocolate-filled\",\n",
        "    \"light-beer\",\n",
        "    \"bagel-without-filling\",\n",
        "    \"oat-milk\",\n",
        "    \"buckwheat-pancake\",\n",
        "    \"gummi-bears-fruit-jellies-jelly-babies-with-fruit-essence\",\n",
        "    \"corn-flakes\",\n",
        "    \"ice-cubes\",\n",
        "    \"black-forest-tart\",\n",
        "    \"m-m-s\",\n",
        "    \"chocolate-milk-chocolate-drink\",\n",
        "    \"cake-marble\",\n",
        "    \"cake-salted\",\n",
        "    \"mango-dried\",\n",
        "    \"blackberry\",\n",
        "    \"italian-salad-dressing\",\n",
        "    \"soup-potato\",\n",
        "    \"brazil-nut\",\n",
        "    \"pastry-flaky\",\n",
        "    \"champagne\",\n",
        "    \"macaroon\",\n",
        "    \"dumplings\",\n",
        "    \"sekt\",\n",
        "    \"soya-drink-soy-milk\",\n",
        "    \"soya-yaourt-yahourt-yogourt-ou-yoghourt\",\n",
        "    \"chorizo\",\n",
        "    \"turnover-with-meat-small-meat-pie-empanadas\",\n",
        "    \"white-chocolate\",\n",
        "    \"margarine\",\n",
        "    \"mix-of-dried-fruits-and-nuts\",\n",
        "    \"white-radish\",\n",
        "    \"grissini\",\n",
        "    \"apricot-dried\",\n",
        "    \"smoked-cooked-sausage-of-pork-and-beef-meat-sausag\",\n",
        "    \"soup-cream-of-vegetables\",\n",
        "    \"prosecco\",\n",
        "    \"soup-miso\",\n",
        "    \"kebab-in-pita-bread\",\n",
        "    \"mushroom-average-stewed-without-addition-of-fat-without-addition-of-salt\",\n",
        "    \"cooked-sausage\",\n",
        "    \"sugar-glazing\",\n",
        "    \"maple-syrup-concentrate\",\n",
        "    \"philadelphia\",\n",
        "    \"aperitif-with-alcohol-aperol-spritz\",\n",
        "    \"damson-plum\",\n",
        "    \"pie-rhubarb-baked-with-cake-dough\",\n",
        "    \"linseeds\",\n",
        "    \"lasagne-vegetable-prepared\",\n",
        "    \"milk-chocolate-with-hazelnuts\",\n",
        "    \"popcorn-salted\",\n",
        "    \"rice-jasmin\",\n",
        "    \"faux-mage-cashew-vegan-chers\",\n",
        "    \"croque-monsieur\",\n",
        "    \"tomato-stewed-without-addition-of-fat-without-addition-of-salt\",\n",
        "    \"cocoa-powder\",\n",
        "    \"perch-fillets-lake\",\n",
        "    \"soup-tomato\",\n",
        "    \"ham-turkey\",\n",
        "    \"fruit-compotes\",\n",
        "    \"french-pizza-from-alsace-baked\",\n",
        "    \"banana-cake\",\n",
        "    \"balsamic-vinegar\",\n",
        "    \"eggplant-caviar\",\n",
        "    \"naan-indien-bread\",\n",
        "    \"chocolate-egg-small\",\n",
        "    \"cake-oblong\",\n",
        "    \"biscuit-with-butter\",\n",
        "    \"pecan-nut\",\n",
        "    \"savoury-puff-pastry-stick\",\n",
        "    \"sweets-candies\",\n",
        "    \"coriander\",\n",
        "    \"fish-crunchies-battered\",\n",
        "    \"chia-grains\",\n",
        "    \"minced-meat\",\n",
        "    \"bean-seeds\",\n",
        "    \"meat-balls\",\n",
        "    \"bouillon-vegetable\",\n",
        "    \"coffee-decaffeinated\",\n",
        "    \"carrot-cake\",\n",
        "    \"paprika-chips\",\n",
        "    \"lemon-pie\",\n",
        "    \"fig-dried\",\n",
        "    \"waffle\"\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_and_filter_classes(drive_path, excluded_set):\n",
        "    \"\"\"\n",
        "    Loads class names from meta.json and filters them based on the hardcoded\n",
        "    EXCLUDED_CATEGORIES set.\n",
        "    \"\"\"\n",
        "    # 1. Load all original class names\n",
        "    meta_path = os.path.join(drive_path, 'meta.json')\n",
        "    if not os.path.exists(meta_path):\n",
        "        raise FileNotFoundError(f\"meta.json not found at {meta_path}\")\n",
        "    with open(meta_path, 'r') as f:\n",
        "        meta_data = json.load(f)\n",
        "    original_class_names = [cls['title'] for cls in meta_data.get('classes', [])]\n",
        "    print(f\"Loaded {len(original_class_names)} total classes from meta.json\")\n",
        "    \n",
        "    # 2. Filter classes using the provided exclusion set\n",
        "    filtered_class_names = [\n",
        "        name for name in original_class_names if name not in excluded_set\n",
        "    ]\n",
        "    num_removed = len(original_class_names) - len(filtered_class_names)\n",
        "    print(f\"Removed {num_removed} specified rare classes.\")\n",
        "    print(f\"Kept {len(filtered_class_names)} classes for training.\")\n",
        "\n",
        "    # 3. Create a new class map for only the filtered classes\n",
        "    filtered_class_map = {name: i for i, name in enumerate(filtered_class_names)}\n",
        "\n",
        "    return filtered_class_names, filtered_class_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 498 total classes from meta.json\n",
            "Removed 88 specified rare classes.\n",
            "Kept 410 classes for training.\n"
          ]
        }
      ],
      "source": [
        "extract_dir = \"/content/extracted_food_recognition\"\n",
        "\n",
        "class_names, class_map = load_and_filter_classes(extract_dir, excluded_food_categories)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Convert annotation json to yolo detection bounding box\n",
        "\n",
        "For example, for `training/img/006497.jpg`, annotations can be found in the relevant annotation folder: `training/ann/006497.jpg.json`\n",
        "\n",
        "However, YOLO needs a label file in text format.\n",
        "\n",
        "You can use this image to better understand how it works: ![yolo sample img](https://github.com/ultralytics/docs/releases/download/0/two-persons-tie.avif)\n",
        "\n",
        "The corresponding label text is as follws:\n",
        "```txt\n",
        "0 0.481719 0.634028 0.690625 0.713278\n",
        "0 0.741094 0.524306 0.314750 0.933389\n",
        "27 0.364844 0.795833 0.078125 0.400000\n",
        "```\n",
        "\n",
        "- The first column (`0`) is the category ID. In this case, `0` refers to `Person`. \n",
        "- The second (`0.481719`) and third columns (`0.634028`) are used as x,y central point coordinates of this object. \n",
        "- The fourth(`0.690625`) and fifth columns(`0.713278`) represent the width and height of the object.\n",
        "\n",
        "\n",
        "For more information, you can check [this documentation](link).\n",
        "\n",
        "**Note:** In our project, the original ID in `meta.json` is not used. The `class_map` returns a unique integer ID as the key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def convert_annotations(split_name, class_map):\n",
        "    print(f\"\\nGenerating labels for {split_name} split using {len(class_map)} classes...\")\n",
        "\n",
        "    split_dir = os.path.join(extract_dir, split_name)\n",
        "    original_ann_dir = os.path.join(split_dir, 'ann')\n",
        "    original_img_dir = os.path.join(split_dir, 'img')\n",
        "\n",
        "    yolo_label_dir = os.path.join(split_dir, 'labels')\n",
        "    os.makedirs(yolo_label_dir, exist_ok=True)\n",
        "\n",
        "    if not os.path.isdir(original_ann_dir):\n",
        "        print(f\"Warning: Annotation dir not found: {original_ann_dir}. Skipping.\")\n",
        "        return\n",
        "\n",
        "    ann_files = [f for f in os.listdir(original_ann_dir) if f.endswith('.json')]\n",
        "\n",
        "    valid_labels = 0\n",
        "    empty_labels = 0\n",
        "\n",
        "    for ann_file in tqdm(ann_files, desc=f'Converting {split_name} annotations'):\n",
        "        json_path = os.path.join(original_ann_dir, ann_file)\n",
        "        with open(json_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        if ann_file.endswith('.jpg.json'):\n",
        "            base_filename = ann_file.replace('.jpg.json', '')\n",
        "            img_extensions = ['.jpg', '.jpeg', '.png', '.JPG']\n",
        "        else:\n",
        "            base_filename = ann_file.replace('.json', '')\n",
        "            img_extensions = ['.jpg', '.jpeg', '.png', '.JPG']\n",
        "\n",
        "        img_path = None\n",
        "        for ext in img_extensions:\n",
        "            potential_path = os.path.join(original_img_dir, f\"{base_filename}{ext}\")\n",
        "            if os.path.exists(potential_path):\n",
        "                img_path = potential_path\n",
        "                break\n",
        "\n",
        "        if not img_path:\n",
        "            print(f\"Warning: No image found for {ann_file}\")\n",
        "            continue\n",
        "\n",
        "        yolo_lines = []\n",
        "        img_h, img_w = data['size']['height'], data['size']['width']\n",
        "        if img_h == 0 or img_w == 0:\n",
        "            continue\n",
        "\n",
        "        for obj in data['objects']:\n",
        "            class_title = obj['classTitle']\n",
        "            if class_title not in class_map:\n",
        "                continue\n",
        "            class_id = class_map[class_title]\n",
        "            points = np.array(obj['points']['exterior'])\n",
        "            if points.shape[0] < 1:\n",
        "                continue\n",
        "            x_min, y_min = points.min(axis=0)\n",
        "            x_max, y_max = points.max(axis=0)\n",
        "            box_w, box_h = x_max - x_min, y_max - y_min\n",
        "            x_center, y_center = x_min + box_w / 2, y_min + box_h / 2\n",
        "            x_norm, y_norm, w_norm, h_norm = x_center / img_w, y_center / img_h, box_w / img_w, box_h / img_h\n",
        "            yolo_lines.append(f\"{class_id} {x_norm:.6f} {y_norm:.6f} {w_norm:.6f} {h_norm:.6f}\")\n",
        "\n",
        "        if yolo_lines:\n",
        "            with open(os.path.join(yolo_label_dir, f\"{base_filename}.txt\"), 'w') as f:\n",
        "                f.write('\\n'.join(yolo_lines))\n",
        "            valid_labels += 1\n",
        "        else:\n",
        "            empty_labels += 1\n",
        "\n",
        "    print(f\"Created {valid_labels} valid label files, {empty_labels} images had no valid annotations\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "convert_annotations(\"training\", class_map)\n",
        "convert_annotations(\"validation\", class_map)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prepare the dataset.yaml\n",
        "\n",
        "For YOLO training, it's important to let YOLO know your training target. In our case, these are food categories that can be found in our image datasets, as well as the path of our training/validation dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset_yaml_path = os.path.join(extract_dir, 'dataset.yaml')\n",
        "\n",
        "final_class_names = list(class_map.keys())\n",
        "yaml_content = {\n",
        "    'path': extract_dir,\n",
        "    'train': os.path.join('training', 'img'),\n",
        "    'val': os.path.join('validation', 'img'),\n",
        "    'nc': len(final_class_names),\n",
        "    'names': final_class_names\n",
        "}\n",
        "with open(dataset_yaml_path, 'w') as f:\n",
        "    yaml.dump(yaml_content, f, sort_keys=False)\n",
        "\n",
        "print(f\"`dataset.yaml` created at: {dataset_yaml_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Step 3: Start Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class FoodDetectionTrainer:\n",
        "    \"\"\"\n",
        "    A comprehensive class to handle the training, evaluation, and testing\n",
        "    of a YOLOv8 food detection model.\n",
        "    \"\"\"\n",
        "    def __init__(self, dataset_path, model_size='n'):\n",
        "        \"\"\"\n",
        "        Initializes the FoodDetectionTrainer.\n",
        "        \n",
        "        Args:\n",
        "            dataset_path (str): The root directory path of the dataset.\n",
        "            model_size (str): The model size ('n', 's', 'm', 'l', 'x').\n",
        "        \"\"\"\n",
        "        self.dataset_path = dataset_path\n",
        "        self.model_size = model_size\n",
        "        self.model_map = {\n",
        "            'n': 'yolov8n.pt',    # Nano - fastest, smallest\n",
        "            's': 'yolov8s.pt',    # Small - balanced\n",
        "            'm': 'yolov8m.pt',    # Medium - better accuracy\n",
        "            'l': 'yolov8l.pt',    # Large - high accuracy\n",
        "            'x': 'yolov8x.pt'     # Extra Large - highest accuracy\n",
        "        }\n",
        "        \n",
        "        # Validate the dataset upon initialization\n",
        "        self.validate_dataset()\n",
        "        \n",
        "    def validate_dataset(self):\n",
        "        \"\"\"Validates the dataset structure and files.\"\"\"\n",
        "        \n",
        "        # Check for dataset.yaml\n",
        "        yaml_path = os.path.join(self.dataset_path, 'dataset.yaml')\n",
        "        if not os.path.exists(yaml_path):\n",
        "            raise FileNotFoundError(f\"dataset.yaml not found in {self.dataset_path}\")\n",
        "        \n",
        "        # Load configuration\n",
        "        with open(yaml_path, 'r') as f:\n",
        "            config = yaml.safe_load(f)\n",
        "        \n",
        "        self.config = config\n",
        "        self.num_classes = config['nc']\n",
        "        print(f\"Detected {self.num_classes} food categories.\")\n",
        "        \n",
        "        # Check training and validation sets\n",
        "        base_path = config.get('path', self.dataset_path)\n",
        "        train_img_path = os.path.join(base_path, config['train'])\n",
        "        val_img_path = os.path.join(base_path, config['val'])\n",
        "        \n",
        "        for split_name, img_path in [('Training set', train_img_path), ('Validation set', val_img_path)]:\n",
        "            if not os.path.exists(img_path):\n",
        "                raise FileNotFoundError(f\"{split_name} image directory not found: {img_path}\")\n",
        "            \n",
        "            # Infer label path from image path\n",
        "            label_path = os.path.join(os.path.dirname(img_path), 'labels')\n",
        "            if not os.path.exists(label_path):\n",
        "                raise FileNotFoundError(f\"{split_name} label directory not found: {label_path}\")\n",
        "            \n",
        "            # Count files\n",
        "            img_files = [f for f in os.listdir(img_path) \n",
        "                        if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "            label_files = [f for f in os.listdir(label_path) if f.endswith('.txt')]\n",
        "            \n",
        "            print(f\"  {split_name}: {len(img_files)} images, {len(label_files)} label files.\")\n",
        "            \n",
        "            valid_labels = 0\n",
        "            empty_labels = 0\n",
        "            for label_file in label_files[:min(50, len(label_files))]:\n",
        "                with open(os.path.join(label_path, label_file), 'r') as f:\n",
        "                    content = f.read().strip()\n",
        "                    if content:\n",
        "                        valid_labels += 1\n",
        "                    else:\n",
        "                        empty_labels += 1\n",
        "            \n",
        "            print(f\"    Sampled Quality - Valid labels: {valid_labels}, Empty labels: {empty_labels}\")\n",
        "        \n",
        "    \n",
        "    def setup_training_config(self, epochs=100, batch_size=16, image_size=640, \n",
        "                            learning_rate=0.01, patience=50):\n",
        "        \n",
        "        if self.num_classes > 200: \n",
        "            recommended_epochs = max(100, min(200, self.num_classes // 2))\n",
        "            recommended_batch = min(batch_size, 32)  \n",
        "            recommended_lr = min(learning_rate, 0.01)  \n",
        "        else:\n",
        "            recommended_epochs = epochs\n",
        "            recommended_batch = batch_size\n",
        "            recommended_lr = learning_rate\n",
        "        \n",
        "        self.training_config = {\n",
        "            'epochs': recommended_epochs,\n",
        "            'batch': recommended_batch,\n",
        "            'imgsz': image_size,\n",
        "            'lr0': recommended_lr,\n",
        "            'patience': patience,\n",
        "            'device': 'auto',\n",
        "            'workers': 8,       \n",
        "            'cache': 'disk',    \n",
        "            'amp': True,        \n",
        "            'optimizer': 'AdamW',\n",
        "            'weight_decay': 0.0005,\n",
        "            'warmup_epochs': 5,\n",
        "            'warmup_momentum': 0.8,\n",
        "            'warmup_bias_lr': 0.1,\n",
        "            'mosaic': 1.0,      # Data augmentation\n",
        "            'mixup': 0.1,\n",
        "            'copy_paste': 0.1,\n",
        "            'degrees': 10.0,    # Rotation\n",
        "            'translate': 0.2,   # Translation\n",
        "            'scale': 0.9,       # Scale\n",
        "            'fliplr': 0.5,      # Horizontal flip\n",
        "            'flipud': 0.0,      # Vertical flip (food is rarely upside down)\n",
        "            'hsv_h': 0.015,     # HSV-Hue augmentation\n",
        "            'hsv_s': 0.7,       # HSV-Saturation augmentation\n",
        "            'hsv_v': 0.4,       # HSV-Value augmentation\n",
        "        }\n",
        "        \n",
        "        \n",
        "        return self.training_config\n",
        "    \n",
        "    def train(self, project_name='food_detection', experiment_name=None):\n",
        "        \"\"\"Starts the training process.\"\"\"\n",
        "        \n",
        "        if experiment_name is None:\n",
        "            experiment_name = f'food_detection_{self.model_size}_{self.num_classes}classes'\n",
        "        \n",
        "        print(f\"\\n Starting food detection model training...\")\n",
        "        print(f\" Project: {project_name}\")\n",
        "        print(f\"Experiment: {experiment_name}\")\n",
        "        \n",
        "        # Load model\n",
        "        model = YOLO(self.model_map[self.model_size])\n",
        "        print(f\"Loaded {self.model_map[self.model_size]} pretrained model.\")\n",
        "        \n",
        "        # Start training\n",
        "        dataset_yaml = os.path.join(self.dataset_path, 'dataset.yaml')\n",
        "        \n",
        "        try:\n",
        "            results = model.train(\n",
        "                data=dataset_yaml,\n",
        "                project=f'runs/detect/{project_name}',\n",
        "                name=experiment_name,\n",
        "                exist_ok=True,\n",
        "                plots=True,\n",
        "                save_json=True,\n",
        "                val=True,\n",
        "                verbose=True,\n",
        "                **self.training_config\n",
        "            )\n",
        "            \n",
        "            self.results = results\n",
        "            self.model = model\n",
        "            \n",
        "            return results\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred during training: {e}\")\n",
        "            raise\n",
        "    \n",
        "    def evaluate(self):\n",
        "        if not hasattr(self, 'results'):\n",
        "            print(\"Please complete training before evaluating!\")\n",
        "            return\n",
        "        \n",
        "        # Run validation\n",
        "        dataset_yaml = os.path.join(self.dataset_path, 'dataset.yaml')\n",
        "        val_results = self.model.val(data=dataset_yaml, plots=True, save_json=True)\n",
        "        \n",
        "        # Display main metrics\n",
        "        metrics = {\n",
        "            'mAP50': val_results.box.map50,\n",
        "            'mAP50-95': val_results.box.map,\n",
        "            'Precision': val_results.box.mp,\n",
        "            'Recall': val_results.box.mr\n",
        "        }\n",
        "        \n",
        "        print(f\"   mAP@0.5: {metrics['mAP50']:.4f} ({metrics['mAP50']*100:.1f}%)\")\n",
        "        print(f\"   mAP@0.5:0.95: {metrics['mAP50-95']:.4f} ({metrics['mAP50-95']*100:.1f}%)\")\n",
        "        print(f\"   Precision: {metrics['Precision']:.4f} ({metrics['Precision']*100:.1f}%)\")\n",
        "        print(f\"   Recall: {metrics['Recall']:.4f} ({metrics['Recall']*100:.1f}%)\")\n",
        "        \n",
        "        # Performance feedback\n",
        "        if metrics['mAP50'] > 0.7:\n",
        "            print(\"\\n Excellent.\")\n",
        "        elif metrics['mAP50'] > 0.5:\n",
        "            print(\"\\n Great.\")\n",
        "        elif metrics['mAP50'] > 0.3:\n",
        "            print(\"\\n Fair.\")\n",
        "        else:\n",
        "            print(\"\\n Needs Improvement.\")\n",
        "        \n",
        "        return val_results\n",
        "    \n",
        "    def visualize_results(self):\n",
        "        if not hasattr(self, 'results'):\n",
        "            print(\"Please complete training before visualizing results!\")\n",
        "            return\n",
        "        \n",
        "        results_dir = self.results.save_dir\n",
        "        \n",
        "        # Display training plots\n",
        "        plots = [\n",
        "            ('results.png', 'Training Curves'),\n",
        "            ('confusion_matrix.png', 'Confusion Matrix'),\n",
        "            ('labels.jpg', 'Label Distribution'),\n",
        "            ('val_batch0_pred.jpg', 'Validation Prediction Sample')\n",
        "        ]\n",
        "        \n",
        "        for filename, title in plots:\n",
        "            filepath = os.path.join(results_dir, filename)\n",
        "            if os.path.exists(filepath):\n",
        "                print(f\"\\n{title}:\")\n",
        "                try:\n",
        "                    display(Image(filepath))\n",
        "                except Exception as e:\n",
        "                    print(f\"Could not display {filename}: {e}\")\n",
        "            else:\n",
        "                print(f\"⚠️ {filename} not found.\")\n",
        "    \n",
        "    def test_prediction(self, test_image_path, confidence=0.25):\n",
        "        \"\"\"Tests a prediction on a single image.\"\"\"\n",
        "        if not hasattr(self, 'model'):\n",
        "            print(\"Please complete training before testing!\")\n",
        "            return\n",
        "        \n",
        "        print(f\"Predicting on image: {test_image_path}\")\n",
        "        \n",
        "        # Make prediction\n",
        "        results = self.model.predict(\n",
        "            test_image_path,\n",
        "            conf=confidence,\n",
        "            save=True,\n",
        "            show_labels=True,\n",
        "            show_conf=True\n",
        "        )\n",
        "        \n",
        "        # Display results\n",
        "        for result in results:\n",
        "            boxes = result.boxes\n",
        "            if boxes and len(boxes) > 0:\n",
        "                print(f\"Detected {len(boxes)} food objects:\")\n",
        "                for i, box in enumerate(boxes):\n",
        "                    class_id = int(box.cls[0])\n",
        "                    confidence_score = float(box.conf[0])\n",
        "                    class_name = self.config['names'][class_id]\n",
        "                    print(f\"  {i+1}. {class_name}: {confidence_score:.3f}\")\n",
        "            else:\n",
        "                print(\"No food objects detected.\")\n",
        "        \n",
        "        return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def main():\n",
        "    \n",
        "    dataset_path = \"/content/extracted_food_recognition\" \n",
        "    trainer = FoodDetectionTrainer(dataset_path, model_size='s')  \n",
        "    \n",
        "    config = trainer.setup_training_config(\n",
        "        epochs=10,        \n",
        "        batch_size=16,    \n",
        "        image_size=640,   \n",
        "        learning_rate=0.01,\n",
        "        patience=30    \n",
        "    )\n",
        "    \n",
        "    results = trainer.train(\n",
        "        project_name='food_detection_project',\n",
        "        experiment_name='food_410classes_v1'\n",
        "    )\n",
        "    \n",
        "    val_results = trainer.evaluate()\n",
        "    \n",
        "    trainer.visualize_results()\n",
        "    \n",
        "    # Test prediction \n",
        "    # test_image = \"/path/to/your/test/image.jpg\"\n",
        "    # if os.path.exists(test_image):\n",
        "    #     trainer.test_prediction(test_image, confidence=0.3)\n",
        "    \n",
        "    return trainer\n",
        "\n",
        "trainer = main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "best_model = trainer.results.save_dir + \"/weights/best.pt\"\n",
        "\n",
        "print(f\"Downloading the best model for local use.')\")\n",
        "\n",
        "files.download(best_model)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
